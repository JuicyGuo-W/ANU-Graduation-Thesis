An Experimental Study of Machine Learning Evaluation Methods
Thesis Submitted for the Degree: Bachelor of Advanced Computing (Honours)
Session: S2/S1 2021–2022
Author: Meilin Guo
Supervisor: Kelvin (Yang) Li

Introduction:
Both theoretical breakthroughs and practical applications of machine learning (ML) are exploding. Unfortunately, there hasn’t been a parallel increase in understanding of how to assess machine learning algorithms. While it’s clear that Deep Learning programmes have beaten the world’s top people in cognitive games like Chess and Go, it’s less clear what makes one machine learning algorithm superior to another in difficult prediction and classification tasks like cancer diagnosis or consumer credit scoring.
We’ll evaluate a range of machine learning methods, including the widely used logistic regression algorithm, neural networks, and decision tree learners, as well as Bayesian network causal discovery techniques (probabilistic graphical models). All of these models are widely used, with proponents often claiming superiority for a variety of learning tasks, generally with just prediction accuracy data to back up their claims. We’ll look into when BIR is better in identifying better learners than predictive accuracy, and when it isn’t. The experimental article will have a strong influence on the machine learning community by laying out the practical evidence that BIR gets closer to the KLD true distribution than predicting accuracy over a wide range of instances.
